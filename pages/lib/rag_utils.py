# lib/rag_utils.py
# -*- coding: utf-8 -*-
"""
bge-m3 ìµœì í™” RAG ìœ í‹¸ (Streamlit ì¹œí™”):
- ì„ë² ë” ë¡œë“œ/ìºì‹œ(get_embedder): GPUë©´ FP16/TF32
- ì„ë² ë”©(embed_texts): bge-m3 ê¶Œì¥ prefix ìë™ ì ìš© (query:/passage:)
- ì˜êµ¬ ë²¡í„° ìŠ¤í† ì–´(get_store): CHROMA_DB_PATH ë˜ëŠ” ./vectorstore
- ì¬ì¸ë±ì‹± ë°©ì§€(upsert_doc): ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ(force=Trueë¡œ ê°•ì œ ê°±ì‹ )
- ê²€ìƒ‰(retrieve_snippets_for_doc): í˜ì´ì§€ ë²”ìœ„ where í•„í„° + query_embeddings ì§ì ‘ ì£¼ì…
- MMR ì¬ìˆœìœ„(_mmr): ê°™ì€ ì„ë² ë”ë¡œ ë¬¸ì„œ ì„ë² ë”© í›„ ë‹¤ì–‘ì„±/ê´€ë ¨ì„± ê· í˜•
- ìš”ì•½(rag_summarize_section): ì»¨í…ìŠ¤íŠ¸ ë°– ì§€ì‹ ê¸ˆì§€ + [#] ì¸ìš©
"""
import os
import hashlib, uuid, math
import numpy as np
import streamlit as st
from typing import List, Tuple, Dict, Optional, Any
import torch
from functools import lru_cache

# ====== Chroma Persistent Store ======
import chromadb

# =========================
# bge-m3 ì„¸íŒ…
# =========================
BGE_MODEL_NAME = "BAAI/bge-m3"
BGE_QUERY_PREFIX = "query: "
BGE_DOC_PREFIX   = "passage: "

try:
    torch.set_float32_matmul_precision("high")
except Exception:
    pass

@lru_cache(maxsize=1)
def get_embedder():
    """SentenceTransformer ì„ë² ë” 1íšŒ ë¡œë“œ + GPU FP16/TF32 ìµœì í™”"""
    from sentence_transformers import SentenceTransformer
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = SentenceTransformer(BGE_MODEL_NAME, device=str(device))

    if device.type == "cuda":
        try:
            # ë°˜ë“œì‹œ GPUë¡œ ì˜¬ë¦¬ê¸°
            model.to(device)
            # (ê°€ëŠ¥ ì‹œ) TF32 í—ˆìš©
            torch.backends.cuda.matmul.allow_tf32 = True
            # (ê°€ëŠ¥ ì‹œ) FP16ë¡œ ì „í™˜ â€” ì‹¤íŒ¨í•´ë„ ì¡°ìš©íˆ FP32 ìœ ì§€
            try:
                model.to(dtype=torch.float16)
            except Exception:
                pass
        except Exception:
            pass
    return model

def embed_texts(
    texts: List[str],
    *,
    task: str = "doc",               # "doc" | "query"
    batch_size: Optional[int] = None,
    normalize: bool = True,
    add_instruction: bool = True,
) -> np.ndarray:
    """texts(list[str]) -> np.ndarray [N, D]  (bge ê¶Œì¥ prefix ìë™ ì ìš©)"""
    if not texts:
        return np.zeros((0, 0), dtype=np.float32)

    if add_instruction:
        if task == "query":
            texts = [BGE_QUERY_PREFIX + (t or "") for t in texts]
        else:
            texts = [BGE_DOC_PREFIX + (t or "") for t in texts]

    emb = get_embedder()
    if batch_size is None:
        batch_size = 128 if torch.cuda.is_available() else 32

    target_device = "cuda" if torch.cuda.is_available() else "cpu"

    with torch.inference_mode():
        vecs = emb.encode(
            texts,
            batch_size=batch_size,
            normalize_embeddings=normalize,
            convert_to_numpy=True,          # ê³„ì‚°ì€ GPUì—ì„œ, ë°˜í™˜ì€ numpy(=CPU ë©”ëª¨ë¦¬)
            show_progress_bar=False,
            device=target_device,           # ğŸ”´ ì¤‘ìš”: encodeì—ë„ device ëª…ì‹œ
        )
    return vecs

# =========================
# Chroma Store
# =========================
def _db_path() -> str:
    return os.getenv("CHROMA_DB_PATH", os.path.abspath("vectorstore"))

@st.cache_resource(show_spinner=False)
def get_store():
    """
    Persistent Chroma + collection (ì¿¼ë¦¬ ì„ë² ë”©ì€ ìš°ë¦¬ê°€ ì§ì ‘ ë„£ì„ ê²ƒì´ë¯€ë¡œ
    embedding_function ë¯¸ì„¤ì •; ë¬¸ì„œ ì—…ì„œíŠ¸ ì‹œì—ë„ precomputed embeddings ì‚¬ìš©)
    """
    client = chromadb.PersistentClient(path=_db_path())
    coll = client.get_or_create_collection(
        name="docs",
        metadata={"hnsw:space": "cosine"},
    )

    # ì™¸ë¶€ í˜¸í™˜ì„ ìœ„í•´ ef ë˜í¼ë¥¼ ì œê³µ(ë‚´ë¶€ì ìœ¼ë¡œ ìš°ë¦¬ embed_texts ì‚¬ìš©)
    def ef(texts: List[str], *, task: str = "doc") -> np.ndarray:
        return embed_texts(texts, task=task, normalize=True, add_instruction=True)

    return client, coll, ef

# =========================
# ì²­í¬
# =========================
def _chunk_page_text(pages: List[Tuple[int, str]], chunk_size=800, overlap=200, doc_id="doc"):
    chunks, metas = [], []
    for pg, txt in (pages or []):
        s = (txt or "").strip()
        if not s:
            continue
        i, cidx, L = 0, 0, len(s)
        while i < L:
            j = min(L, i + chunk_size)
            ch = s[i:j]
            chunks.append(ch)
            metas.append({"doc_id": doc_id, "page": pg, "chunk": f"p{pg}_c{cidx}"})
            cidx += 1
            if j == L:
                break
            i = max(0, j - overlap)
    return chunks, metas

def _sha1_bytes(b: bytes) -> str:
    return hashlib.sha1(b).hexdigest()[:16]

# =========================
# ì¡´ì¬ ì—¬ë¶€
# =========================
def doc_exists(doc_id: str) -> bool:
    _, coll, _ = get_store()
    try:
        res = coll.get(where={"doc_id": doc_id}, limit=1)
    except TypeError:
        res = coll.get(where={"doc_id": doc_id})
    return bool(res.get("ids"))

# =========================
# ì—…ì„œíŠ¸ (ì¬ì¸ë±ì‹± ë°©ì§€)
# =========================
def upsert_doc(
    pages_text: List[Tuple[int, str]],
    source_name: str,
    *,
    chunk_size=800,
    overlap=200,
    file_bytes: Optional[bytes] = None,
    force: bool = False,     # âœ… ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ. ê°•ì œ ê°±ì‹  ì‹œ True
):
    client, coll, _ = get_store()

    # ë¬¸ì„œ UID: íŒŒì¼ë°”ì´íŠ¸ ìš°ì„ , ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ê¸°ë°˜
    doc_uid_bytes = file_bytes or "\n".join((t or "") for _, t in pages_text).encode("utf-8")
    doc_id = _sha1_bytes(doc_uid_bytes)

    # ì´ë¯¸ ì¸ë±ì‹±ë˜ì–´ ìˆìœ¼ë©´ ìŠ¤í‚µ
    if (not force) and doc_exists(doc_id):
        return 0, doc_id

    # force=Trueë©´ ê¸°ì¡´ ë¬¸ì„œ ì œê±° í›„ ìƒˆë¡œ ì—…ì„œíŠ¸
    if force:
        try:
            coll.delete(where={"doc_id": doc_id})
        except Exception:
            pass

    # ì²­í¬ ìƒì„±
    chunks, metas = _chunk_page_text(
        pages_text, chunk_size=chunk_size, overlap=overlap, doc_id=doc_id
    )
    if not chunks:
        return 0, doc_id

    ids = [f"{doc_id}_{i+1}" for i in range(len(chunks))]
    metas = [{**m, "source": source_name} for m in metas]

    # ì„ë² ë”© (ì§„í–‰ë¥  í‘œì‹œ)
    B = 128 if torch.cuda.is_available() else 64
    vecs = []
    prog = st.progress(0.0, text=f"ì„ë² ë”© ê³„ì‚° ì¤‘â€¦ (ë°°ì¹˜ {B})")
    total_batches = (len(chunks) + B - 1) // B
    for bi in range(0, len(chunks), B):
        batch = chunks[bi:bi+B]
        v = embed_texts(batch, task="doc", add_instruction=True)
        vecs.append(v)
        done = (bi // B) + 1
        prog.progress(done / total_batches)
    prog.empty()
    embs = np.vstack(vecs).tolist()

    # ë²¡í„°DB ì €ì¥ (ì§„í–‰ë¥  í‘œì‹œ)
    B2 = 256
    prog2 = st.progress(0.0, text="ë²¡í„°DBì— ì €ì¥ ì¤‘â€¦")
    total_batches2 = (len(chunks) + B2 - 1) // B2
    for bi in range(0, len(chunks), B2):
        coll.add(
            ids=ids[bi:bi+B2],
            documents=chunks[bi:bi+B2],
            metadatas=metas[bi:bi+B2],
            embeddings=embs[bi:bi+B2],
        )
        done2 = (bi // B2) + 1
        prog2.progress(done2 / total_batches2)
    prog2.empty()

    return len(chunks), doc_id

# =========================
# ê²€ìƒ‰ + MMR
# =========================
def _build_where(doc_id: str, page_range: Optional[Tuple[int, int]]):
    # Chroma ìµœì‹  í•„í„°: ë£¨íŠ¸ì— í•˜ë‚˜ì˜ ì—°ì‚°ìë§Œ í—ˆìš©
    if page_range and len(page_range) == 2:
        s, e = int(page_range[0]), int(page_range[1])
        return {
            "$and": [
                {"doc_id": doc_id},         # ë™ë“± ë¹„êµëŠ” ì´ë ‡ê²Œ ì¨ë„ ë¨
                {"page": {"$gte": s}},      # ë²”ìœ„ ì¡°ê±´ì„ ê°œë³„ í•­ëª©ìœ¼ë¡œ ë¶„ë¦¬
                {"page": {"$lte": e}},
            ]
        }
    # ë‹¨ì¼ ì¡°ê±´ì´ë¼ë„ ì¼ê´€ì„± ìœ„í•´ $andë¡œ ê°ì‹¸ë„ ë¬´ë°©
    return {"$and": [{"doc_id": doc_id}]}

def _l2n(x: np.ndarray):
    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-9
    return x / n

def _mmr(docs: List[Tuple[str, Dict]], query: str, top_k=8, lambda_mult=0.6):
    """
    docs: [(text, meta), ...]
    ê°™ì€ ì„ë² ë”ë¡œ ë¬¸ì„œ/ì§ˆì˜ ì„ë² ë”© â†’ ê´€ë ¨ì„±/ë‹¤ì–‘ì„± ê· í˜•
    """
    texts = [d[0] for d in docs]
    if not texts:
        return []
    E = _l2n(embed_texts(texts, task="doc", add_instruction=True))
    q = _l2n(embed_texts([query], task="query", add_instruction=True))[0]
    rel = E @ q
    selected = [int(np.argmax(rel))]
    cand = set(range(len(texts))) - set(selected)

    while len(selected) < min(top_k, len(texts)) and cand:
        best, best_score = None, -1e9
        for i in list(cand):
            div = float(np.max(E[i] @ E[selected].T))
            score = lambda_mult * float(rel[i]) - (1 - lambda_mult) * div
            if score > best_score:
                best, best_score = i, score
        selected.append(best)
        cand.remove(best)

    return [docs[i] for i in selected]

def retrieve_snippets_for_doc(
    doc_id: str,
    query: str,
    *,
    n_initial=40,
    top_k=18,
    page_range: Optional[Tuple[int, int]] = None,
    soft_expand: bool = True,
) -> List[Tuple[str, Dict]]:
    """
    í˜ì´ì§€ ë²”ìœ„(where) ìš°ì„  ê²€ìƒ‰ â†’ (ë¶€ì¡± ì‹œ) ë¬¸ì„œ ì „ì²´ë¡œ ë³´ì¶© â†’ MMR ì¬ìˆœìœ„
    return: [(text, metadata), ...]
    """
    _, coll, _ = get_store()

    # ì§ˆì˜ ì„ë² ë”©ì„ ìš°ë¦¬ê°€ ì§ì ‘ ë„£ìŒ(ì¼ê´€ íŒŒì´í”„ë¼ì¸)
    qvec = embed_texts([query], task="query", add_instruction=True)[0]

    # 1) ì„ íƒ êµ¬ê°„ë§Œ ê²€ìƒ‰
    where = _build_where(doc_id, page_range)
    res = coll.query(
        query_embeddings=[qvec],
        n_results=min(n_initial, 100),
        where=where,
        include=["documents", "metadatas", "distances"],
    )
    docs = list(zip(res.get("documents", [[]])[0], res.get("metadatas", [[]])[0]))

    # 2) ë²”ìœ„ê°€ ë¹ˆì•½í•˜ë©´ ë¬¸ì„œ ì „ì²´ë¡œ ë³´ì¶©(ì¤‘ë³µ ì œê±°)
    if soft_expand and len(docs) < max(6, top_k // 2):
        res_all = coll.query(
            query_embeddings=[qvec],
            n_results=min(n_initial, 100),
            where={"doc_id": doc_id},
            include=["documents", "metadatas", "distances"],
        )
        docs_all = list(zip(res_all.get("documents", [[]])[0], res_all.get("metadatas", [[]])[0]))
        seen, merged = set(), []
        for pair in docs + docs_all:
            key = (pair[0], tuple(sorted(pair[1].items())))
            if key not in seen:
                seen.add(key)
                merged.append(pair)
        docs = merged

    if not docs:
        return []

    # 3) MMR ì¬ìˆœìœ„
    return _mmr(docs, query, top_k=top_k, lambda_mult=0.6)

# =========================
# ì»¨í…ìŠ¤íŠ¸ ìœ í‹¸
# =========================
def cap_by_chars(snips: List[Tuple[str, Dict]], max_chars=12000):
    out, used = [], 0
    for t, m in snips:
        L = len(t or "")
        if used + L <= max_chars:
            out.append((t, m))
            used += L
        else:
            r = max_chars - used
            if r > 400:
                out.append(((t or "")[:r], m))
            break
    return out

def format_context(snips: List[Tuple[str, Dict]]):
    blocks = []
    for i, (t, m) in enumerate(snips, start=1):
        tag = f"[{i}] ({m.get('source','?')} p.{m.get('page','?')} {m.get('chunk','?')})"
        blocks.append(f"{tag}\n{t}")
    return "\n\n---\n\n".join(blocks)

# =========================
# RAG ìš”ì•½
# =========================
def rag_summarize_section(
    client: Any,
    model: str,
    doc_id: str,
    query: str,
    page_range: Tuple[int, int],
    *,
    max_chars_context: int = 9000,
    top_k: int = 16,
) -> Dict[str, Any]:
    """
    ì„ íƒ êµ¬ê°„ë§Œ ìš”ì•½ (ë¶€ì¡± ì‹œ soft expand ë³´ì¡°)
    returns: {"summary": str, "evidence": List[(text, meta)]}
    """
    snips = retrieve_snippets_for_doc(
        doc_id, query, n_initial=40, top_k=top_k, page_range=page_range, soft_expand=True
    )
    if not snips:
        return {
            "summary": f"ì„ íƒí•œ êµ¬ê°„(p.{page_range[0]}â€“{page_range[1]})ì—ì„œ ìš”ì•½í•  í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "evidence": [],
        }

    snips = cap_by_chars(snips, max_chars=max_chars_context)
    ctx = format_context(snips)

    system = (
        "You are a focused summarizer for a study app.\n"
        "Use ONLY the provided context. Do NOT add external knowledge.\n"
        "Cite evidence using [#] indices that match the context blocks."
    )
    user = f"""
[Task]
Summarize ONLY the selected page range p.{page_range[0]}â€“{page_range[1]} for the query:
{query}

[Style]
- Bullet points (3â€“7 bullets)
- Be concise and faithful to the context
- Each important claim ends with a citation like [3] or [1][4]
- If the answer isn't in the context, say so.

[Context]
{ctx}
"""

    resp = client.chat.completions.create(
        model=model,
        messages=[
            {"role":"system", "content": system},
            {"role":"user",   "content": user},
        ],
        temperature=0.2,
    )
    summary = resp.choices[0].message.content.strip()
    return {"summary": summary, "evidence": snips}
